{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.2\n"
     ]
    }
   ],
   "source": [
    "#Author: Caleb Woy\n",
    "!python --version\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "### Loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Potato</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Potato</td>\n",
       "      <td>potato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potato</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Potato</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Potato</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Document    Word\n",
       "0   Potato     the\n",
       "1   Potato  potato\n",
       "2   Potato      is\n",
       "3   Potato       a\n",
       "4   Potato    root"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 2000\n",
    "word_data = pd.read_excel('TextMiningWords1.xlsx')\n",
    "word_data = word_data.drop(columns=['Index'])\n",
    "word_data['Word'] = word_data['Word'].astype(str)\n",
    "word_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466\n"
     ]
    }
   ],
   "source": [
    "# removing sweet potatoes document\n",
    "word_data_no_sp = word_data[word_data.Document != 'SweetPotato']\n",
    "print(len(word_data_no_sp))\n",
    "\n",
    "def Clean(data):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    # removing stopwords\n",
    "    stopwords = set(stopwords.words('english'))\n",
    "    data = data[~data['Word'].isin(stopwords)]\n",
    "    # removing rows where Word contains a number\n",
    "    data = data[~data['Word'].str.contains(r'\\d')]\n",
    "    # removing possesive nouns\n",
    "    data = data[~data['Word'].str.contains(r\".'s$\")]\n",
    "    # Removing countries\n",
    "    country_list = set(['united', 'states'\n",
    "                       , 'mexico', 'us', 'americas', 'peru',\n",
    "                      'chile', 'bolivia', 'andes', 'europe', 'asia',\n",
    "                       'china', 'india','kingdom', 'world', \"world's\"])\n",
    "    data = data[~data['Word'].isin(country_list)]\n",
    "    #replacing maize with corn\n",
    "    data = data.replace({'maize':'corn'})\n",
    "    # transforming plural to singlular\n",
    "    # standardizing tense\n",
    "    data['Word'] = data['Word'].apply(lambda x: WordNetLemmatizer().lemmatize(x))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n"
     ]
    }
   ],
   "source": [
    "word_data_no_sp = Clean(word_data_no_sp)\n",
    "print(len(word_data_no_sp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination and observations\n",
    "\n",
    "I removed the default list of stopwords from nltk so that we can get more accurate similrity measures.\n",
    "I removed numbers because I don't think they're important given the context of our text.\n",
    "I removed possexive nouns because thaty're usually countries or people. Not super important for our context.\n",
    "I removed countries because they're not important for our context. I left in tokens like 'Spanish' or 'American' because they're specifically descriptive.\n",
    "I replaced maize with corn because only corn.txt called it maize.\n",
    "I used the wordnet lemmatizer to standardize tense and plurality. It replaces words with their root meaning.\n",
    "\n",
    "In total this cleaning process reduced our data by 662 words. Preprocessing is important for emphasizing a context of focus. It'll make it easier to compare our documents in the way that we really care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Term Frequency Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating weights with double normalization\n",
    "counts = word_data_no_sp.groupby(['Document', 'Word']).size()\n",
    "docs = word_data_no_sp['Document'].unique().tolist()\n",
    "tfweights = {x:{} for x in docs}\n",
    "for doc in docs:\n",
    "    max_in_doc = max(counts[doc])\n",
    "    for word, count in counts[doc].iteritems():\n",
    "        tfweights[doc][word] = 0.5 + (0.5 * count / max_in_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Potato\n",
      "     Word: potato, \t\tWeight: 1.0\n",
      "     Word: food, \t\tWeight: 0.6428571428571428\n",
      "     Word: specie, \t\tWeight: 0.6428571428571428\n",
      "     Word: human, \t\tWeight: 0.6071428571428571\n",
      "     Word: part, \t\tWeight: 0.6071428571428571\n",
      "     Word: plant, \t\tWeight: 0.6071428571428571\n",
      "     Word: production, \t\tWeight: 0.6071428571428571\n",
      "     Word: solanum, \t\tWeight: 0.6071428571428571\n",
      "     Word: southern, \t\tWeight: 0.6071428571428571\n",
      "     Word: tuber, \t\tWeight: 0.6071428571428571\n",
      "Document: PotatoChip\n",
      "     Word: potato, \t\tWeight: 1.0\n",
      "     Word: chip, \t\tWeight: 0.8888888888888888\n",
      "     Word: recipe, \t\tWeight: 0.7222222222222222\n",
      "     Word: book, \t\tWeight: 0.6666666666666666\n",
      "     Word: fried, \t\tWeight: 0.6666666666666666\n",
      "     Word: market, \t\tWeight: 0.6666666666666666\n",
      "     Word: slice, \t\tWeight: 0.6666666666666666\n",
      "     Word: snack, \t\tWeight: 0.6666666666666666\n",
      "     Word: billion, \t\tWeight: 0.6111111111111112\n",
      "     Word: british, \t\tWeight: 0.6111111111111112\n",
      "Document: Tomato\n",
      "     Word: tomato, \t\tWeight: 1.0\n",
      "     Word: plant, \t\tWeight: 0.75\n",
      "     Word: aztec, \t\tWeight: 0.6428571428571428\n",
      "     Word: spanish, \t\tWeight: 0.6071428571428571\n",
      "     Word: used, \t\tWeight: 0.6071428571428571\n",
      "     Word: word, \t\tWeight: 0.6071428571428571\n",
      "     Word: america, \t\tWeight: 0.5714285714285714\n",
      "     Word: annual, \t\tWeight: 0.5714285714285714\n",
      "     Word: berry, \t\tWeight: 0.5714285714285714\n",
      "     Word: commonly, \t\tWeight: 0.5714285714285714\n",
      "Document: CornChip\n",
      "     Word: chip, \t\tWeight: 1.0\n",
      "     Word: corn, \t\tWeight: 0.9615384615384616\n",
      "     Word: made, \t\tWeight: 0.6153846153846154\n",
      "     Word: tortilla, \t\tWeight: 0.6153846153846154\n",
      "     Word: aroma, \t\tWeight: 0.5769230769230769\n",
      "     Word: bag, \t\tWeight: 0.5769230769230769\n",
      "     Word: chili, \t\tWeight: 0.5769230769230769\n",
      "     Word: flavor, \t\tWeight: 0.5769230769230769\n",
      "     Word: le, \t\tWeight: 0.5769230769230769\n",
      "     Word: popular, \t\tWeight: 0.5769230769230769\n",
      "Document: Corn\n",
      "     Word: corn, \t\tWeight: 1.0\n",
      "     Word: used, \t\tWeight: 0.58\n",
      "     Word: ethanol, \t\tWeight: 0.56\n",
      "     Word: grain, \t\tWeight: 0.56\n",
      "     Word: grown, \t\tWeight: 0.56\n",
      "     Word: human, \t\tWeight: 0.56\n",
      "     Word: also, \t\tWeight: 0.54\n",
      "     Word: animal, \t\tWeight: 0.54\n",
      "     Word: called, \t\tWeight: 0.54\n",
      "     Word: crop, \t\tWeight: 0.54\n",
      "Document: TortillaChip\n",
      "     Word: corn, \t\tWeight: 1.0\n",
      "     Word: made, \t\tWeight: 0.8333333333333333\n",
      "     Word: tortilla, \t\tWeight: 0.8333333333333333\n",
      "     Word: baked, \t\tWeight: 0.6666666666666666\n",
      "     Word: chip, \t\tWeight: 0.6666666666666666\n",
      "     Word: food, \t\tWeight: 0.6666666666666666\n",
      "     Word: fried, \t\tWeight: 0.6666666666666666\n",
      "     Word: also, \t\tWeight: 0.5833333333333334\n",
      "     Word: alternatively, \t\tWeight: 0.5833333333333334\n",
      "     Word: although, \t\tWeight: 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "for doc in docs:\n",
    "    print(f'Document: {doc}')\n",
    "    top_words = collections.Counter(tfweights[doc])\n",
    "    for key, value in top_words.most_common(10):\n",
    "        print(f'     Word: {key}, \\t\\tWeight: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination and Observations\n",
    "\n",
    "I calculated the raw term counts with pandas built in groupby function and size function.\n",
    "I calculated the weights with double normalization because I noticed the documents vary in length.\n",
    "\n",
    "A lot of the weights are the same are the top 4-5. This is due to a lot of the less frequent words only being mentioned once.\n",
    "\n",
    "The tfweights will be important for calculating similarity later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Inverse Document Frequency Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(docs)\n",
    "wrds = word_data_no_sp['Word'].unique().tolist()\n",
    "idfweights = {}\n",
    "for wrd in wrds:\n",
    "    k = 0\n",
    "    for doc in docs:\n",
    "        if wrd in tfweights[doc]:\n",
    "            k += 1\n",
    "    idfweights[wrd] = math.log(n / k, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word: food, \t\tWeight: 0.0\n",
      "     Word: ingredient, \t\tWeight: 0.0791812460476248\n",
      "     Word: known, \t\tWeight: 0.17609125905568124\n",
      "     Word: may, \t\tWeight: 0.17609125905568124\n",
      "     Word: corn, \t\tWeight: 0.17609125905568124\n",
      "     Word: part, \t\tWeight: 0.17609125905568124\n",
      "     Word: many, \t\tWeight: 0.17609125905568124\n",
      "     Word: year, \t\tWeight: 0.17609125905568124\n",
      "     Word: variety, \t\tWeight: 0.17609125905568124\n",
      "     Word: also, \t\tWeight: 0.30102999566398114\n",
      "     Word: usually, \t\tWeight: 0.30102999566398114\n",
      "     Word: oil, \t\tWeight: 0.30102999566398114\n",
      "     Word: made, \t\tWeight: 0.30102999566398114\n",
      "     Word: widely, \t\tWeight: 0.30102999566398114\n",
      "     Word: first, \t\tWeight: 0.30102999566398114\n",
      "     Word: salt, \t\tWeight: 0.30102999566398114\n",
      "     Word: flavor, \t\tWeight: 0.30102999566398114\n",
      "     Word: including, \t\tWeight: 0.30102999566398114\n",
      "     Word: dish, \t\tWeight: 0.30102999566398114\n",
      "     Word: snack, \t\tWeight: 0.30102999566398114\n",
      "     Word: baked, \t\tWeight: 0.30102999566398114\n",
      "     Word: fried, \t\tWeight: 0.30102999566398114\n",
      "     Word: english, \t\tWeight: 0.30102999566398114\n",
      "     Word: often, \t\tWeight: 0.30102999566398114\n",
      "     Word: chip, \t\tWeight: 0.30102999566398114\n",
      "     Word: produce, \t\tWeight: 0.30102999566398114\n",
      "     Word: grown, \t\tWeight: 0.30102999566398114\n",
      "     Word: production, \t\tWeight: 0.30102999566398114\n",
      "     Word: wheat, \t\tWeight: 0.30102999566398114\n",
      "     Word: crop, \t\tWeight: 0.30102999566398114\n"
     ]
    }
   ],
   "source": [
    "top_words = collections.Counter(idfweights)\n",
    "for key, value in reversed(top_words.most_common()[-30:]):\n",
    "    print(f'     Word: {key}, \\t\\tWeight: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination and Observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculated the IDF weights utilizing the speed of dictionary indexing to check if the tfweight dict contained a specific word. If the dict contained a word then a counter was increased. I passed the result of that counter to the IDF function as k. \n",
    "\n",
    "A lot of the top words don't have to do with food however most are still useful. The only ones which probably aren't are 'known', 'may', 'part', 'many', 'also', 'usually', 'including', and 'often'. These could be added to our collection of stopwords if necessary. \n",
    "\n",
    "Inverse document frequencies are important to compare with frequencies within documents. If a majority of occurences across all documents are concentrated in one document that gives us a clue about the significance of that word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate TF-IDF Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfxidf(tf, idf):\n",
    "    for word in idf:\n",
    "        for doc in tf:\n",
    "            if word in tf[doc]:\n",
    "                tf[doc][word] = tf[doc][word] * idf[word]\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfxidf(tfweights, idfweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Potato\n",
      "     Word: potato, \t\tWeight: 0.47712125471966244\n",
      "     Word: tuber, \t\tWeight: 0.4724489734472121\n",
      "     Word: eastern, \t\tWeight: 0.44465785736208197\n",
      "     Word: enough, \t\tWeight: 0.44465785736208197\n",
      "     Word: glycoalkaloids, \t\tWeight: 0.44465785736208197\n",
      "     Word: health, \t\tWeight: 0.44465785736208197\n",
      "     Word: region, \t\tWeight: 0.44465785736208197\n",
      "     Word: still, \t\tWeight: 0.44465785736208197\n",
      "     Word: wild, \t\tWeight: 0.44465785736208197\n",
      "     Word: accumulate, \t\tWeight: 0.4168667412769519\n",
      "Document: PotatoChip\n",
      "     Word: recipe, \t\tWeight: 0.5619981252770758\n",
      "     Word: book, \t\tWeight: 0.5187675002557623\n",
      "     Word: market, \t\tWeight: 0.5187675002557623\n",
      "     Word: slice, \t\tWeight: 0.5187675002557623\n",
      "     Word: potato, \t\tWeight: 0.47712125471966244\n",
      "     Word: british, \t\tWeight: 0.4755368752344489\n",
      "     Word: call, \t\tWeight: 0.4755368752344489\n",
      "     Word: dripping, \t\tWeight: 0.4755368752344489\n",
      "     Word: large, \t\tWeight: 0.4755368752344489\n",
      "     Word: peel, \t\tWeight: 0.4755368752344489\n",
      "Document: Tomato\n",
      "     Word: aztec, \t\tWeight: 0.5002400895323422\n",
      "     Word: tomato, \t\tWeight: 0.47712125471966244\n",
      "     Word: word, \t\tWeight: 0.4724489734472121\n",
      "     Word: annual, \t\tWeight: 0.44465785736208197\n",
      "     Word: berry, \t\tWeight: 0.44465785736208197\n",
      "     Word: height, \t\tWeight: 0.44465785736208197\n",
      "     Word: time, \t\tWeight: 0.44465785736208197\n",
      "     Word: typically, \t\tWeight: 0.44465785736208197\n",
      "     Word: according, \t\tWeight: 0.4168667412769519\n",
      "     Word: across, \t\tWeight: 0.4168667412769519\n",
      "Document: CornChip\n",
      "     Word: aroma, \t\tWeight: 0.4489334136828712\n",
      "     Word: bag, \t\tWeight: 0.4489334136828712\n",
      "     Word: chili, \t\tWeight: 0.4489334136828712\n",
      "     Word: le, \t\tWeight: 0.4489334136828712\n",
      "     Word: popular, \t\tWeight: 0.4489334136828712\n",
      "     Word: rigid, \t\tWeight: 0.4489334136828712\n",
      "     Word: alone, \t\tWeight: 0.4190045194373465\n",
      "     Word: brand, \t\tWeight: 0.4190045194373465\n",
      "     Word: commercial, \t\tWeight: 0.4190045194373465\n",
      "     Word: common, \t\tWeight: 0.4190045194373465\n",
      "Document: Corn\n",
      "     Word: ethanol, \t\tWeight: 0.43576470021484043\n",
      "     Word: grain, \t\tWeight: 0.43576470021484043\n",
      "     Word: animal, \t\tWeight: 0.4202016752071675\n",
      "     Word: called, \t\tWeight: 0.4202016752071675\n",
      "     Word: feed, \t\tWeight: 0.4202016752071675\n",
      "     Word: inflorescence, \t\tWeight: 0.4202016752071675\n",
      "     Word: kernel, \t\tWeight: 0.4202016752071675\n",
      "     Word: million, \t\tWeight: 0.4202016752071675\n",
      "     Word: sweet, \t\tWeight: 0.4202016752071675\n",
      "     Word: ton, \t\tWeight: 0.4202016752071675\n",
      "Document: TortillaChip\n",
      "     Word: alternatively, \t\tWeight: 0.4539215627237921\n",
      "     Word: although, \t\tWeight: 0.4539215627237921\n",
      "     Word: angeles, \t\tWeight: 0.4539215627237921\n",
      "     Word: blue, \t\tWeight: 0.4539215627237921\n",
      "     Word: colouring, \t\tWeight: 0.4539215627237921\n",
      "     Word: commercially, \t\tWeight: 0.4539215627237921\n",
      "     Word: cuisine, \t\tWeight: 0.4539215627237921\n",
      "     Word: disc, \t\tWeight: 0.4539215627237921\n",
      "     Word: glutamate, \t\tWeight: 0.4539215627237921\n",
      "     Word: grew, \t\tWeight: 0.4539215627237921\n"
     ]
    }
   ],
   "source": [
    "# displaying top 10 words per document\n",
    "for doc in tfidf:\n",
    "    print(f'Document: {doc}')\n",
    "    top_words = collections.Counter(tfidf[doc])\n",
    "    for key, value in top_words.most_common(10):\n",
    "        print(f'     Word: {key}, \\t\\tWeight: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word: potato, \t\tWeight: 0.9542425094393249\n",
      "     Word: chip, \t\tWeight: 0.7692988778079518\n",
      "     Word: tomato, \t\tWeight: 0.7327219268909102\n",
      "     Word: tortilla, \t\tWeight: 0.6912141254272033\n",
      "     Word: corn, \t\tWeight: 0.6221246130373793\n",
      "     Word: made, \t\tWeight: 0.592643155566166\n",
      "     Word: used, \t\tWeight: 0.5664110895314849\n",
      "     Word: plant, \t\tWeight: 0.5650763061463875\n",
      "     Word: fried, \t\tWeight: 0.5634664021402724\n",
      "     Word: specie, \t\tWeight: 0.562321478776745\n",
      "     Word: recipe, \t\tWeight: 0.5619981252770758\n",
      "     Word: human, \t\tWeight: 0.5568686644370917\n",
      "     Word: total, \t\tWeight: 0.5492195776550781\n",
      "     Word: solanum, \t\tWeight: 0.5452814339653285\n",
      "     Word: similar, \t\tWeight: 0.5433880956529489\n",
      "     Word: cut, \t\tWeight: 0.5433880956529489\n",
      "     Word: well, \t\tWeight: 0.5433880956529489\n",
      "     Word: billion, \t\tWeight: 0.5396771525606849\n",
      "     Word: snack, \t\tWeight: 0.5383805691682739\n",
      "     Word: southern, \t\tWeight: 0.5377838142483052\n",
      "     Word: commonly, \t\tWeight: 0.5377080807158101\n",
      "     Word: red, \t\tWeight: 0.5339214040910508\n",
      "     Word: baked, \t\tWeight: 0.5300186248442744\n",
      "     Word: america, \t\tWeight: 0.5282413891539119\n",
      "     Word: originated, \t\tWeight: 0.5282413891539119\n",
      "     Word: masa, \t\tWeight: 0.5264237843740276\n",
      "     Word: crunchy, \t\tWeight: 0.5219788085821948\n",
      "     Word: domesticated, \t\tWeight: 0.5207437694368887\n",
      "     Word: found, \t\tWeight: 0.5206680359043936\n",
      "     Word: second, \t\tWeight: 0.5206680359043936\n"
     ]
    }
   ],
   "source": [
    "# displaying top 30 words overall by sum\n",
    "word_sums = {word: 0 for word in idfweights}\n",
    "for word in word_sums:\n",
    "    for doc in tfidf:\n",
    "        if word in tfidf[doc]:\n",
    "            word_sums[word] += tfidf[doc][word]\n",
    "top_words = collections.Counter(word_sums)\n",
    "for key, value in top_words.most_common(30):\n",
    "    print(f'     Word: {key}, \\t\\tWeight: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Word: recipe, \t\tWeight: 0.5619981252770758\n",
      "     Word: slice, \t\tWeight: 0.5187675002557623\n",
      "     Word: market, \t\tWeight: 0.5187675002557623\n",
      "     Word: book, \t\tWeight: 0.5187675002557623\n",
      "     Word: aztec, \t\tWeight: 0.5002400895323422\n",
      "     Word: british, \t\tWeight: 0.4755368752344489\n",
      "     Word: thin, \t\tWeight: 0.4755368752344489\n",
      "     Word: large, \t\tWeight: 0.4755368752344489\n",
      "     Word: shaving, \t\tWeight: 0.4755368752344489\n",
      "     Word: peel, \t\tWeight: 0.4755368752344489\n",
      "     Word: round, \t\tWeight: 0.4755368752344489\n",
      "     Word: dripping, \t\tWeight: 0.4755368752344489\n",
      "     Word: call, \t\tWeight: 0.4755368752344489\n",
      "     Word: tuber, \t\tWeight: 0.4724489734472121\n",
      "     Word: word, \t\tWeight: 0.4724489734472121\n",
      "     Word: triangle, \t\tWeight: 0.4539215627237921\n",
      "     Word: alternatively, \t\tWeight: 0.4539215627237921\n",
      "     Word: disc, \t\tWeight: 0.4539215627237921\n",
      "     Word: pressed, \t\tWeight: 0.4539215627237921\n",
      "     Word: water, \t\tWeight: 0.4539215627237921\n",
      "     Word: although, \t\tWeight: 0.4539215627237921\n",
      "     Word: mass-produced, \t\tWeight: 0.4539215627237921\n",
      "     Word: commercially, \t\tWeight: 0.4539215627237921\n",
      "     Word: los, \t\tWeight: 0.4539215627237921\n",
      "     Word: angeles, \t\tWeight: 0.4539215627237921\n",
      "     Word: late, \t\tWeight: 0.4539215627237921\n",
      "     Word: grew, \t\tWeight: 0.4539215627237921\n",
      "     Word: mexican, \t\tWeight: 0.4539215627237921\n",
      "     Word: cuisine, \t\tWeight: 0.4539215627237921\n",
      "     Word: item, \t\tWeight: 0.4539215627237921\n"
     ]
    }
   ],
   "source": [
    "# displaying top 30 words overall by mult\n",
    "word_maxs = {word: 0 for word in idfweights}\n",
    "for word in word_maxs:\n",
    "    for doc in tfidf:\n",
    "        if word in tfidf[doc]:\n",
    "            if tfidf[doc][word] > word_maxs[word]:\n",
    "                word_maxs[word] = tfidf[doc][word]\n",
    "top_words = collections.Counter(word_mults)\n",
    "for key, value in top_words.most_common(30):\n",
    "    print(f'     Word: {key}, \\t\\tWeight: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(['corn', 'potato', 'chip', 'tomato', 'tortilla', 'made',\n",
    "           'aztec', 'plant', 'fried', 'recipe', 'american', 'spanish',\n",
    "           'glycoalkaloids', 'tuber', 'berry', 'grew', 'british', 'annual',\n",
    "           'blue', 'angeles', 'cuisine', 'chili', 'popular', 'wild',\n",
    "           'production', 'animal', 'peel', 'eastern', 'ethanol', 'masa'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination and observations\n",
    "\n",
    "I calculated the tfidf weights by simulating matrix multiplication using my dictionaries. I listed the top 10 weights for each document using the Counter object. I displayed the top 30 overall terms by both the highest sum tfidf weights and the highest max tfidf weights. I chose words for the vocabulary by picking the words that seemed the most relevant with regard to context from the top 10 terms per document. I then picked a couple more from the top overall displayed by SUM words.\n",
    "\n",
    "The top overall terms by sum and by max tfidf are very different lists. The sum method likely better emphasizes top overall words spread across documents. The max method probably emphasizes the top overall terms that are unique to only few documents. I chose to use some words from the sum method list because I thought it would add some bias to the vocabulary. \n",
    "\n",
    "Selcted vocabularies are important for measuring simularity with a focus as we'll see in the next stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Document Similarity Model and Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PotatoxPotatoChip similarity: 0.227644691705265\n",
      "PotatoxTomato similarity: 0.22216517098618632\n",
      "PotatoxCornChip similarity: 0.017037434898799473\n",
      "PotatoxCorn similarity: 0.10128264961738599\n",
      "PotatoxTortillaChip similarity: 0.01771893229475145\n",
      "\n",
      "PotatoChipxPotato similarity: 0.227644691705265\n",
      "PotatoChipxTomato similarity: 0.0\n",
      "PotatoChipxCornChip similarity: 0.11308019239539016\n",
      "PotatoChipxCorn similarity: 0.0\n",
      "PotatoChipxTortillaChip similarity: 0.09397531970758452\n",
      "\n",
      "TomatoxPotato similarity: 0.22216517098618632\n",
      "TomatoxPotatoChip similarity: 0.0\n",
      "TomatoxCornChip similarity: 0.0\n",
      "TomatoxCorn similarity: 0.09016596299800922\n",
      "TomatoxTortillaChip similarity: 0.0\n",
      "\n",
      "CornChipxPotato similarity: 0.017037434898799473\n",
      "CornChipxPotatoChip similarity: 0.11308019239539016\n",
      "CornChipxTomato similarity: 0.0\n",
      "CornChipxCorn similarity: 0.05881360972552516\n",
      "CornChipxTortillaChip similarity: 0.2859703144439937\n",
      "\n",
      "CornxPotato similarity: 0.10128264961738599\n",
      "CornxPotatoChip similarity: 0.0\n",
      "CornxTomato similarity: 0.09016596299800922\n",
      "CornxCornChip similarity: 0.05881360972552516\n",
      "CornxTortillaChip similarity: 0.13932861325850993\n",
      "\n",
      "TortillaChipxPotato similarity: 0.01771893229475145\n",
      "TortillaChipxPotatoChip similarity: 0.09397531970758452\n",
      "TortillaChipxTomato similarity: 0.0\n",
      "TortillaChipxCornChip similarity: 0.2859703144439937\n",
      "TortillaChipxCorn similarity: 0.13932861325850993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_weights = {doc: {} for doc in tfidf}\n",
    "for word in vocab:\n",
    "    for doc in tfidf:\n",
    "        if word in tfidf[doc]:\n",
    "            vocab_weights[doc][word] = tfidf[doc][word]\n",
    "        else:\n",
    "            vocab_weights[doc][word] = 0\n",
    "            \n",
    "doc_sims = {doc: {} for doc in tfidf}\n",
    "for doc in doc_sims:\n",
    "    doc_sims[doc] = {doc2: 0 for doc2 in tfidf if doc2 != doc}\n",
    "    \n",
    "for doc1 in doc_sims:\n",
    "    for doc2 in doc_sims[doc1]:\n",
    "        sim = 0\n",
    "        for word in vocab:\n",
    "            sim += vocab_weights[doc1][word] * vocab_weights[doc2][word]\n",
    "        doc_sims[doc1][doc2] = sim\n",
    "        print(f'{doc1}x{doc2} similarity: {sim}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesized catergories: Chips, Corn-based\n",
    "\n",
    "**Corn-based**: Corn, CornChip, TortillaChip<br/>\n",
    "**Non-corn-based**: Potato, Tomato, PotatoChip<br/>\n",
    "\n",
    "### Analysis\n",
    "\n",
    "PotatoxPotatoChip similarity: 0.227644691705265<br/>\n",
    "PotatoxTomato similarity: 0.22216517098618632<br/>\n",
    "PotatoxCornChip similarity: 0.017037434898799473<br/>\n",
    "PotatoxCorn similarity: 0.10128264961738599<br/>\n",
    "PotatoxTortillaChip similarity: 0.01771893229475145<br/>\n",
    "\n",
    "The similarity measures between Potato and the other documents support our proposed categories. Potato is most similar to Tomato and Potatochip and least similar to the corn based documents.\n",
    "\n",
    "PotatoChipxPotato similarity: 0.227644691705265<br/>\n",
    "PotatoChipxTomato similarity: 0.0<br/>\n",
    "PotatoChipxCornChip similarity: 0.11308019239539016<br/>\n",
    "PotatoChipxCorn similarity: 0.0<br/>\n",
    "PotatoChipxTortillaChip similarity: 0.09397531970758452<br/>\n",
    "\n",
    "PotatoChip is most similar to Potato, CornChip, and TortillaChip. Perhaps there should be a 'Chip vs Crop' dimension to this too.\n",
    "\n",
    "TomatoxPotato similarity: 0.22216517098618632<br/>\n",
    "TomatoxPotatoChip similarity: 0.0<br/>\n",
    "TomatoxCornChip similarity: 0.0<br/>\n",
    "TomatoxCorn similarity: 0.09016596299800922<br/>\n",
    "TomatoxTortillaChip similarity: 0.0<br/>\n",
    "\n",
    "Tomato is most similar to Potato and Corn. Both crops. 0 similarity to the chips.\n",
    "\n",
    "CornChipxPotato similarity: 0.017037434898799473<br/>\n",
    "CornChipxPotatoChip similarity: 0.11308019239539016<br/>\n",
    "CornChipxTomato similarity: 0.0<br/>\n",
    "CornChipxCorn similarity: 0.05881360972552516<br/>\n",
    "CornChipxTortillaChip similarity: 0.2859703144439937<br/>\n",
    "\n",
    "CornChip is most similar to TortillaChip, PotatoChip and Corn. Again, seeing evidence of the chip dimension here.\n",
    "\n",
    "CornxPotato similarity: 0.10128264961738599<br/>\n",
    "CornxPotatoChip similarity: 0.0<br/>\n",
    "CornxTomato similarity: 0.09016596299800922<br/>\n",
    "CornxCornChip similarity: 0.05881360972552516<br/>\n",
    "CornxTortillaChip similarity: 0.13932861325850993<br/>\n",
    "\n",
    "Corn is most similar to TortillaChip. Corn is actually more similar to Potato and Tomato than to CornChip. The Chip vs Crop dimension has shown up more in our similarity measure than our proposed hypotheses.\n",
    "\n",
    "TortillaChipxPotato similarity: 0.01771893229475145<br/>\n",
    "TortillaChipxPotatoChip similarity: 0.09397531970758452<br/>\n",
    "TortillaChipxTomato similarity: 0.0<br/>\n",
    "TortillaChipxCornChip similarity: 0.2859703144439937<br/>\n",
    "TortillaChipxCorn similarity: 0.13932861325850993<br/>\n",
    "\n",
    "TortillaChip is most similar to CornChip and Corn. This is fitting with our proposed categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination and Observations\n",
    "\n",
    "I got the simularity calculations by simulating all pairwise dot products between the documents using my dictionaries.\n",
    "\n",
    "I think the term 'tortilla' is what made Corn and TortillaChip have higher similarity than CornChip and Corn. I would have expected the opposite. My proposed categories held up pretty well but I think a second dimension of Crop vs Chip would also be necessary with this vocabulary. \n",
    "\n",
    "The simularity matrix calcuation is important for mining clusters/categories of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "This process seems similar to ANOVA-like processes that focus on between vs within group variation except in the document mining process we're using likeklihood of word observation rather than variance. I think selecting the vocabulary to be used in the simularity calculation is very important for teasing out the proper result. It likely causes a lot of variation, we could test this by comparing against a randomly selected vocabulary. If I were to redo the project I'd try using another simularity calculation vs just the dot product to seew how it changes my result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
